<html>
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=gb2312">
  <title>乡下小老鼠日常笔记</title>
  <link href="TryDoxygen.css" rel="stylesheet" type="text/css">
  <meta name="robots" content="index,nofollow">
</head>
<table width="100%" bgColor="#99CCFF">
<tr align=left>
	<td align=left height=46 class="style1"><a href="index.html">
<img class="logo" src="logo.png" height="46" alt="" 
            style="background-color: #99CCFF; background-image: none;" /></a></td> <td align=center>
        <font size=6 color="#FFFFFF" face=""><b>乡下小老鼠日常笔记</b></font>
	</td><td align=left height=46 class="style1">
<img class="space" src="space.png" height="46" alt="" 
            style="background-color: #99CCFF; background-image: none;" /></td>
</tr> </table>
<!-- 制作者 Doxygen 1.8.15 -->
</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Zookeeper理论与搭建 </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><hr/>
 <p>顾名思义zookeeper就是动物园管理员，他是用来管hadoop（大象）、Hive(蜜蜂)、pig(小猪)的管理员， Apache Hbase和 Apache Solr 的分布式集群都用到了zookeeper；Zookeeper:是一个分布式的、开源的程序协调服务，是hadoop项目下的一个子项目。它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。</p>
<p>Zookeeper 不仅可以单机提供服务，同时也支持多机组成集群来提供服务。实际上 Zookeeper 还支持另外一种伪集群的方式，也就是可以在一台物理机上运行多个 Zookeeper 实例。下面介绍集群模式的安装和配置。</p>
<p>ZooKeeper集群中具有两个关键的角色：Leader和Follower。集群中所有的结点作为一个整体对分布式应用提供服务，集群中每个结点之间都互相连接，所以，在配置的ZooKeeper集群的时候，每一个结点的host到IP地址的映射都要配置上集群中其它结点的映射信息。</p>
<p>ZooKeeper采用一种称为Leader election的选举算法。在整个集群运行过程中，只有一个Leader，其他的都是Follower，如果ZooKeeper集群在运行过程中Leader出了问题，系统会采用该算法重新选出一个Leader。因此，各个结点之间要能够保证互相连接，必须配置上述映射。</p>
<h1><a class="anchor" id="zkfun"></a>
zookeeper主要功能</h1>
<h2><a class="anchor" id="cfgmng"></a>
配置管理</h2>
<p>在我们的应用中除了代码外，还有一些就是各种配置。比如数据库连接等。一般我们都是使用配置文件的方式，在代码中引入这些配置文件。但是当我们只有一种配置，只有一台服务器，并且不经常修改的时候，使用配置文件是一个很好的做法，但是如果我们配置非常多，有很多服务器都需要这个配置，而且还可能是动态的话使用配置文件就不是个好主意了。这个时候往往需要寻找一种集中管理配置的方法，我们在这个集中的地方修改了配置，所有对这个配置感兴趣的都可以获得变更。比如我们可以把配置放在数据库里，然后所有需要配置的服务都去这个数据库读取配置。但是，因为很多服务的正常运行都非常依赖这个配置，所以需要这个集中提供配置服务的服务具备很高的可靠性。一般我们可以用一个集群来提供这个配置服务，但是用集群提升可靠性，那如何保证配置在集群中的一致性呢？ 这个时候就需要使用一种实现了一致性协议的服务了。Zookeeper就是这种服务，它使用Zab这种一致性协议来提供一致性。现在有很多开源项目使用Zookeeper来维护配置，比如在HBase中，客户端就是连接一个Zookeeper，获得必要的HBase集群的配置信息，然后才可以进一步操作。还有在开源的消息队列Kafka中，也使用Zookeeper来维护broker的信息。在Alibaba开源的SOA框架Dubbo中也广泛的使用Zookeeper管理一些配置来实现服务治理。</p>
<h2><a class="anchor" id="nser"></a>
名字服务</h2>
<p>名字服务这个就很好理解了。比如为了通过网络访问一个系统，我们得知道对方的IP地址，但是IP地址对人非常不友好，这个时候我们就需要使用域名来访问。但是计算机是不能是别域名的。怎么办呢？如果我们每台机器里都备有一份域名到IP地址的映射，这个倒是能解决一部分问题，但是如果域名对应的IP发生变化了又该怎么办呢？于是我们有了DNS这个东西。我们只需要访问一个大家熟知的(known)的点，它就会告诉你这个域名对应的IP是什么。在我们的应用中也会存在很多这类问题，特别是在我们的服务特别多的时候，如果我们在本地保存服务的地址的时候将非常不方便，但是如果我们只需要访问一个大家都熟知的访问点，这里提供统一的入口，那么维护起来将方便得多了。</p>
<h2><a class="anchor" id="dislock"></a>
分布式锁</h2>
<p>其实在第一篇文章中已经介绍了Zookeeper是一个分布式协调服务。这样我们就可以利用Zookeeper来协调多个分布式进程之间的活动。比如在一个分布式环境中，为了提高可靠性，我们的集群的每台服务器上都部署着同样的服务。但是，一件事情如果集群中的每个服务器都进行的话，那相互之间就要协调，编程起来将非常复杂。而如果我们只让一个服务进行操作，那又存在单点。通常还有一种做法就是使用分布式锁，在某个时刻只让一个服务去干活，当这台服务出问题的时候锁释放，立即fail over到另外的服务。这在很多分布式系统中都是这么做，这种设计有一个更好听的名字叫Leader Election(leader选举)。比如HBase的Master就是采用这种机制。但要注意的是分布式锁跟同一个进程的锁还是有区别的，所以使用的时候要比同一个进程里的锁更谨慎的使用。</p>
<h2><a class="anchor" id="clusmng"></a>
集群管理</h2>
<p>在分布式的集群中，经常会由于各种原因，比如硬件故障，软件故障，网络问题，有些节点会进进出出。有新的节点加入进来，也有老的节点退出集群。这个时候，集群中其他机器需要感知到这种变化，然后根据这种变化做出对应的决策。比如我们是一个分布式存储系统，有一个中央控制节点负责存储的分配，当有新的存储进来的时候我们要根据现在集群目前的状态来分配存储节点。这个时候我们就需要动态感知到集群目前的状态。还有，比如一个分布式的SOA架构中，服务是一个集群提供的，当消费者访问某个服务时，就需要采用某种机制发现现在有哪些节点可以提供该服务(这也称之为服务发现，比如Alibaba开源的SOA框架Dubbo就采用了Zookeeper作为服务发现的底层机制)。还有开源的Kafka队列就采用了Zookeeper作为Cosnumer的上下线管理。</p>
<h1><a class="anchor" id="dissample"></a>
Zookeeper 部署示例</h1>
<h2><a class="anchor" id="IPana"></a>
IP规划与初始配置</h2>
<p>分别使用IP为172.16.0.123，172.16.0.128和172.16.0.131的三台机器进行搭建，在这三台机器上添加相同的用户（如：solradmin <b>后文的用户名都使用了solradmin，如使用自己的用户名请做相应修改</b>）。 </p>
<h2><a class="anchor" id="downanin"></a>
下载二进制版本，解压安装</h2>
<p>从http://zookeeper.apache.org/下载当前最新稳定版本（3.4.9），并解压安装：</p>
<pre class="fragment">cd ~/Downloads/
wget http://mirrors.cnnic.cn/apache/zookeeper/stable/zookeeper-3.4.9.tar.gz
tar zxf zookeeper-3.4.9.tar.gz
mv zookeeper-3.4.9/ /usr/local/zookeeper
</pre> <h2><a class="anchor" id="cfgenfs"></a>
配置环境变量与关闭防火墙端口</h2>
<p>在/etc/profile.d下面创建一个文件zookeeper.sh，并输入以下内容：</p>
<pre class="fragment"># set zookeeper environment
export ZOOKEEPER_HOME=/usr/local/zookeeper
export PATH=$PATH:$ZOOKEEPER_HOME/bin
export CLASSPATH=$CLASSPATH:$ZOOKEEPER_HOME/lib
</pre><p> 执行# source /etc/profile ,使环境变量立即生效。分别编辑三台机器上的/etc/sysconfig/iptables，增加以下信息，以开启端口2181、2888和3888 </p><pre class="fragment">-A INPUT -m state --state NEW -m tcp -p tcp --dport 2181 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 2888 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 3888 -j ACCEPT
</pre> <h2><a class="anchor" id="cfgzk"></a>
配置zookeeper</h2>
<p>创建路径用于存放Zookeeper的数据和日志信息 </p><pre class="fragment">mkdir -p /data/zookeeper/{data,log}
cp /usr/local/zookeeper/conf/zoo_sample.cfg /usr/local/zookeeper/conf/zoo.cfg
</pre><p> 修改 内容如下： </p><pre class="fragment"># The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial 
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between
# sending a request and getting an acknowledgement 
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just 
# example sakes.
dataDir=/data/zookeeper/data
# the port at which the clients will connect   
clientPort=2181
# the maximum number of client connections. 
# increase this if you need to handle more clients
#maxClientCnxns=60 
#
# Be sure to read the maintenance section of the 
# administrator guide before turning on autopurge. 
#
# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance   
#
# The number of snapshots to retain in dataDir  
#autopurge.snapRetainCount=3  
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval=1
server.1=172.16.0.123:2888:3888
server.2=172.16.0.128:2888:3888
server.3=172.16.0.131:2888:3888
</pre> <p>这个配置文件中各个配置项的意义:</p>
<ul>
<li>tickTime：这个时间是作为 Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。</li>
<li>initLimit：这个配置项是用来配置 Zookeeper 接受客户端（这里所说的客户端不是用户连接 Zookeeper 服务器的客户端，而是 Zookeeper 服务器集群中连接到 Leader 的 Follower 服务器）初始化连接时最长能忍受多少个心跳时间间隔数。当已经超过 5 个心跳的时间（也就是 tickTime）长度后 Zookeeper 服务器还没有收到客户端的返回信息，那么表明这个客户端连接失败。总的时间长度就是 5*2000=10 秒</li>
<li>syncLimit：这个配置项标识 Leader 与 Follower 之间发送消息，请求和应答时间长度，最长不能超过多少个 tickTime 的时间长度，总的时间长度就是 2*2000=4 秒</li>
<li>dataDir：顾名思义就是 Zookeeper 保存数据的目录，默认情况下，Zookeeper 将写数据的日志文件也保存在这个目录里。</li>
<li>clientPort：这个端口就是客户端连接 Zookeeper 服务器的端口，Zookeeper 会监听这个端口，接受客户端的访问请求。</li>
<li>server.A=B：C：D：其中 A 是一个数字，表示这个是第几号服务器；B 是这个服务器的 ip 地址；C 表示的是这个服务器与集群中的 Leader 服务器交换信息的端口；D 表示的是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的 Leader，而这个端口就是用来执行选举时服务器相互通信的端口。如果是伪集群的配置方式，由于 B 都是一样，所以不同的 Zookeeper 实例通信端口号不能一样，所以要给它们分配不同的端口号。 </li>
</ul>
<p>上述配置内容说明，可以参考http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html::sc_RunningReplicatedZooKeeper。</p>
<p>除了修改 zoo.cfg 配置文件，集群模式下还要配置一个文件 myid，这个文件在 dataDir 目录下，这个文件里面就有一个数据就是 A 的值，Zookeeper 启动时会读取这个文件，拿到里面的数据与 zoo.cfg 里面的配置信息比较从而判断到底是那个 server。在ip为172.16.0.123的机器上执行以下语句，其余机器类似。</p>
<pre class="fragment">echo 1 &gt; /data/zookeeper/data/myid
</pre> <p>修改/usr/local/zookeeper/bin/zkEnv.sh文件的</p>
<pre class="fragment">if [ "x${ZOO_LOG_DIR}" = "x" ]
then
    ZOO_LOG_DIR=""
fi
...
if [ "$JAVA_HOME" != "" ]; then
  JAVA="$JAVA_HOME/bin/java"
else
  JAVA=java
fi
</pre><p> 为 </p><pre class="fragment">if [ "x${ZOO_LOG_DIR}" = "x" ]
then
    ZOO_LOG_DIR="/data/zookeeper/log/"
fi
...
if [ "$JAVA_HOME" != "" ]; then
  JAVA="$JAVA_HOME/bin/java"
else
  JAVA=/usr/java/jdk1.7.0_67-cloudera/bin/java  #真实的java路径
fi
</pre> <h2><a class="anchor" id="rostaer"></a>
开机自动运行</h2>
<p>在/etc/init.d目录下增加zkSolr文件，并加个可执行权限： </p><pre class="fragment">cd /etc/init.d
touch zkSolr
chmod +x zkSolr
</pre><p> 再修改zkSolr的内容为： </p><pre class="fragment">#!/bin/bash 
#chkconfig: 2345 20 80   
# description:  zkSolr
case $1 in
          start) su solradmin /usr/local/zookeeper/bin/zkServer.sh start;;
          stop) su solradmin /usr/local/zookeeper/bin/zkServer.sh stop;;
          status) su solradmin /usr/local/zookeeper/bin/zkServer.sh status;;
          restart) su solradmin /usr/local/zookeeper/bin/zkServer.sh restart;;
          *)  echo "require start|stop|status|restart";;
esac
</pre><p> 最后用chkconfig &ndash;add zkSolr 增加服务</p>
<h2><a class="anchor" id="omdis"></a>
其他机器的部署配置</h2>
<pre class="fragment">scp /etc/profile.d/hisigncloud.sh root@172.16.0.128:/etc/profile.d/
scp -r /usr/local/zookeeper root@172.16.0.128:/usr/local/
scp /etc/init.d/zkSolr root@172.16.0.128:/etc/init.d/
scp /etc/profile.d/hisigncloud.sh root@172.16.0.131:/etc/profile.d/
scp -r /usr/local/zookeeper root@172.16.0.131:/usr/local/
scp /etc/init.d/zkSolr root@172.16.0.131:/etc/init.d/
</pre><p> 分别登录两台机器，运行：source /etc/profile 和 mkdir -p /data/zookeeper/{data,log} 创建zookeeper的数据路径，运行chkconfig &ndash;add zkSolr 增加服务。在172.16.0.128上执行：echo 2 &gt; /data/zookeeper/data/myid，在172.16.0.131上执行：echo 3 &gt; /data/zookeeper/data/myid 在所有节点上执行 $ service zkSolr start，分别启动，然后用zkServer.sh status查看启动状况。 </p>
</div></div><!-- PageDoc -->
</div><!-- contents -->
<hr>
<MARQUEE behavior="alternate">乡下小老鼠倾力打造 &reg</MARQUEE>
</html>
